{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Facial Expression Recognition Project\n",
        "# Problem Statement- Model for identify the emotion being expressed by a person's face."
      ],
      "metadata": {
        "id": "8FBZeoxKJDZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#import scikitplot\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.utils import np_utils\n"
      ],
      "metadata": {
        "id": "0v-hQYeBJBMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install np_utils\n"
      ],
      "metadata": {
        "id": "DkGuZublqwYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikitplot"
      ],
      "metadata": {
        "id": "IGVLvSHbqTL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gathering Data"
      ],
      "metadata": {
        "id": "ZrgTigBzJaYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RXF2sjnsnSa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/MINI/fer2013/fer2013.csv')"
      ],
      "metadata": {
        "id": "6WtV06CSnSdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Preprocessing"
      ],
      "metadata": {
        "id": "0lfTgwIeJeQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LUdtftbEnSgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.emotion.unique()"
      ],
      "metadata": {
        "id": "Uo0oufeEnSjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}"
      ],
      "metadata": {
        "id": "T3_N8KUonSmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.emotion.value_counts()"
      ],
      "metadata": {
        "id": "ueDQK1aNnSpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(df.emotion)\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "ysZLKLFEmg7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt(len(df.pixels[0].split(' ')))"
      ],
      "metadata": {
        "id": "mDS48LU0mg9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = pyplot.figure(1, (14, 14))\n",
        "\n",
        "k = 0\n",
        "for label in sorted(df.emotion.unique()):\n",
        "    for j in range(7):\n",
        "        px = df[df.emotion==label].pixels.iloc[k]\n",
        "        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n",
        "\n",
        "        k += 1\n",
        "        ax = pyplot.subplot(7, 7, k)\n",
        "        ax.imshow(px, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(emotion_label_to_text[label])\n",
        "        pyplot.tight_layout()"
      ],
      "metadata": {
        "id": "IyniJqD7mhBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INTERESTED_LABELS = [3, 4, 6]"
      ],
      "metadata": {
        "id": "BQhzrhBQmhEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.emotion.isin(INTERESTED_LABELS)]\n",
        "df.shape"
      ],
      "metadata": {
        "id": "GjAh9En1r-gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\n",
        "img_array = np.stack(img_array, axis=0)"
      ],
      "metadata": {
        "id": "TOVtyazRr-jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_array.shape"
      ],
      "metadata": {
        "id": "B0PMcBhhr-mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "img_labels = le.fit_transform(df.emotion)\n",
        "img_labels = np_utils.to_categorical(img_labels)\n",
        "img_labels.shape"
      ],
      "metadata": {
        "id": "soIhoXaWr-ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(le_name_mapping)"
      ],
      "metadata": {
        "id": "sXgwGcaor-rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training, Evalation, selection, development"
      ],
      "metadata": {
        "id": "R1XJMI2VJjtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels,\n",
        "                                                    shuffle=True, stratify=img_labels,\n",
        "                                                    test_size=0.1, random_state=42)\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
      ],
      "metadata": {
        "id": "_JzYl4LKr-uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df\n",
        "del img_array\n",
        "del img_labels"
      ],
      "metadata": {
        "id": "OZc3LiElr-xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = X_train.shape[1]\n",
        "img_height = X_train.shape[2]\n",
        "img_depth = X_train.shape[3]\n",
        "num_classes = y_train.shape[1]"
      ],
      "metadata": {
        "id": "_N5t6X_Or-0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing results, as neural networks are very sensitive to unnormalized data.\n",
        "X_train = X_train / 255.\n",
        "X_valid = X_valid / 255."
      ],
      "metadata": {
        "id": "Th6hhrnzmhG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_net(optim):\n",
        "    \"\"\"\n",
        "    This is a Deep Convolutional Neural Network (DCNN). For generalization purpose I used dropouts in regular intervals.\n",
        "    I used `ELU` as the activation because it avoids dying relu problem but also performed well as compared to LeakyRelu\n",
        "    atleast in this case. `he_normal` kernel initializer is used as it suits ELU. BatchNormalization is also used for better\n",
        "    results.\n",
        "    \"\"\"\n",
        "    net = Sequential(name='DCNN')\n",
        "\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(5,5),\n",
        "            input_shape=(img_width, img_height, img_depth),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_1'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_1'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(5,5),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_2'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_2'))\n",
        "\n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n",
        "    net.add(Dropout(0.4, name='dropout_1'))\n",
        "\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_3'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_3'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_4'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_4'))\n",
        "\n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n",
        "    net.add(Dropout(0.4, name='dropout_2'))\n",
        "\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_5'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_5'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_6'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_6'))\n",
        "\n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n",
        "    net.add(Dropout(0.5, name='dropout_3'))\n",
        "\n",
        "    net.add(Flatten(name='flatten'))\n",
        "\n",
        "    net.add(\n",
        "        Dense(\n",
        "            128,\n",
        "            activation='elu',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='dense_1'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_7'))\n",
        "\n",
        "    net.add(Dropout(0.6, name='dropout_4'))\n",
        "\n",
        "    net.add(\n",
        "        Dense(\n",
        "            num_classes,\n",
        "            activation='softmax',\n",
        "            name='out_layer'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    net.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=optim,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    net.summary()\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "OJBPfUJPmhJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "I used two callbacks one is `early stopping` for avoiding overfitting training data\n",
        "and other `ReduceLROnPlateau` for learning rate.\n",
        "\"\"\"\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.00005,\n",
        "    patience=11,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=7,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    early_stopping,\n",
        "    lr_scheduler,\n",
        "]"
      ],
      "metadata": {
        "id": "MCguGxzXmhNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As the data in hand is less as compared to the task so ImageDataGenerator is good to go.\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "train_datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "WF2Qpkyjs-ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 #batch size of 32 performs the best.\n",
        "epochs = 100\n",
        "optims = [\n",
        "    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n",
        "    optimizers.Adam(0.001),\n",
        "]\n",
        "\n",
        "# I tried both `Nadam` and `Adam`, the difference in results is not different but I finally went with Nadam as it is more popular.\n",
        "model = build_net(optims[1])\n",
        "history = model.fit_generator(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    steps_per_epoch=len(X_train) / batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    use_multiprocessing=True\n",
        ")"
      ],
      "metadata": {
        "id": "bHR0L5sws-Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "L16D3WcNs-Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set()\n",
        "fig = pyplot.figure(0, (12, 4))\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 1)\n",
        "sns.lineplot(history.epoch, history.history['accuracy'], label='train')\n",
        "sns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 2)\n",
        "sns.lineplot(history.epoch, history.history['loss'], label='train')\n",
        "sns.lineplot(history.epoch, history.history['val_loss'], label='valid')\n",
        "pyplot.title('Loss')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "pyplot.savefig('epoch_history_dcnn.png')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "7urzG05ms-UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\n",
        "df_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n",
        "\n",
        "fig = pyplot.figure(0, (14, 4))\n",
        "ax = pyplot.subplot(1, 2, 1)\n",
        "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 2)\n",
        "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\n",
        "pyplot.title('Loss')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "pyplot.savefig('performance_dist.png')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "7DqhddwDs-Wo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}